import os
import json
from tqdm import tqdm
from datetime import datetime
from google.generativeai import GenerativeModel, configure

# ==== Configurations ====
API_KEY = "AIzaSyBq4HTkU_PWUyHh7NmOuFPSjgzMQI86CCo"
TEMPERATURE = 0.7
N_SAMPLE = 100  # Total samples per category
SAVE_DIR = "generated_qna_data"
COMBINED_FILE = "combined_qna.json"

# ==== Category Mapping ====
CATEGORY_LABELS = {
    'chinh-sach': 'Ch√≠nh s√°ch',
    'thue': 'Thu·∫ø',
    'tai-chinh': 'T√†i ch√≠nh',
    'an-ninh-quoc-gia': 'An ninh qu·ªëc gia',
    'hanh-chinh': 'H√†nh ch√≠nh',
    'tu-phap': 'T∆∞ ph√°p',
    'doanh-nghiep': 'Doanh nghi·ªáp',
    'dat-dai': 'ƒê·∫•t ƒëai',
    'y-te': 'Y t·∫ø',
    'an-ninh-trat-tu': 'An ninh tr·∫≠t t·ª±',
    'dau-tu': 'ƒê·∫ßu t∆∞',
    'co-cau-to-chuc': 'C∆° c·∫•u t·ªï ch·ª©c',
    'tai-nguyen': 'T√†i nguy√™n',
    'giao-thong': 'Giao th√¥ng',
    'giao-duc': 'Gi√°o d·ª•c',
    'lao-dong': 'Lao ƒë·ªông',
    'thong-tin': 'Th√¥ng tin',
    'xay-dung': 'X√¢y d·ª±ng',
    'van-hoa': 'VƒÉn h√≥a',
    'cong-nghiep': 'C√¥ng nghi·ªáp',
    'ngoai-giao': 'Ngo·∫°i giao',
    'nong-nghiep': 'N√¥ng nghi·ªáp',
    'thuong-mai': 'Th∆∞∆°ng m·∫°i',
    'hinh-su': 'H√¨nh s·ª±',
    'khieu-nai': 'Khi·∫øu n·∫°i',
    'khoa-hoc': 'Khoa h·ªçc',
    'quoc-phong': 'Qu·ªëc ph√≤ng',
    'xuat-nhap-canh': 'Xu·∫•t nh·∫≠p c·∫£nh',
    'can-bo': 'C√°n b·ªô',
    'dan-su': 'D√¢n s·ª±',
    'dau-thau': 'ƒê·∫•u th·∫ßu',
    'ke-toan': 'K·∫ø to√°n',
    'so-huu-tri-tue': 'S·ªü h·ªØu tr√≠ tu·ªá',
    'bao-hiem': 'B·∫£o hi·ªÉm',
    'hai-quan': 'H·∫£i quan',
    'hang-hai': 'H√†ng h·∫£i',
    'hon-nhan-gia-dinh': 'H√¥n nh√¢n gia ƒë√¨nh',
    'thi-dua': 'Thi ƒëua',
    'tiet-kiem': 'Ti·∫øt ki·ªám',
    'vi-pham-hanh-chinh': 'Vi ph·∫°m h√†nh ch√≠nh',
    'chung-khoan': 'Ch·ª©ng kho√°n',
    'cu-tru': 'C∆∞ tr√∫',
    'toa-an': 'T√≤a √°n',
    'xuat-nhap-khau': 'Xu·∫•t nh·∫≠p kh·∫©u'
}

# ==== Gemini Initialization ====
configure(api_key=API_KEY)
model = GenerativeModel("gemini-1.5-pro")

# ==== Helper Functions ====

def build_prompt(categories, n_samples):
    n_each_length = n_samples // 3
    prompt = "H√£y t·∫°o danh s√°ch c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn c√°c lƒ©nh v·ª±c ph√°p lu·∫≠t sau. M·ªói lƒ©nh v·ª±c ph·∫£i c√≥ c√°c c√¢u h·ªèi ƒë·∫∑c tr∆∞ng r√µ r√†ng, kh√¥ng tr√πng l·∫∑p √Ω v·ªõi c√°c lƒ©nh v·ª±c kh√°c. C√¢u h·ªèi ph·∫£i th·ª±c t·∫ø, ph√π h·ª£p v·ªõi lƒ©nh v·ª±c ƒë√≥.\n\n"

    prompt += f"V·ªõi m·ªói lƒ©nh v·ª±c, h√£y t·∫°o:\n- {n_each_length} c√¢u h·ªèi ng·∫Øn (d∆∞·ªõi 15 t·ª´)\n- {n_each_length} c√¢u h·ªèi trung b√¨nh (15-30 t·ª´)\n- {n_each_length} c√¢u h·ªèi d√†i (tr√™n 30 t·ª´)\n\n"

    prompt += "Danh s√°ch lƒ©nh v·ª±c:\n"
    for slug, name in categories.items():
        prompt += f"- {name}\n"

    prompt += "\nƒê·ªãnh d·∫°ng tr·∫£ l·ªùi:\n[category_name]\n- c√¢u h·ªèi 1\n- c√¢u h·ªèi 2\n...\n\nB·∫Øt ƒë·∫ßu nh√©."
    
def generate_all_categories(categories, n_samples, temperature):
    response = model.generate_content(build_prompt(categories, n_samples), generation_config={"temperature": temperature})
    return response.text.strip() if response.text else ""

def parse_gemini_output(raw_text, categories):
    data = {slug: [] for slug in categories.keys()}
    current_category = None

    for line in raw_text.splitlines():
        line = line.strip()
        if not line:
            continue

        if line.startswith("[") and line.endswith("]"):
            name = line[1:-1].strip()
            slug = next((k for k, v in categories.items() if v == name), None)
            current_category = slug
            continue

        if current_category and line.startswith("- "):
            question = line[2:].strip()
            if question:
                data[current_category].append({"question": question, "category": current_category})

    return data

def save_json(data, path):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_json(path):
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []

# ==== Main Process ====
os.makedirs(SAVE_DIR, exist_ok=True)
combined_data = load_json(os.path.join(SAVE_DIR, COMBINED_FILE))

# Exclude 'linh-vuc-khac'
target_categories = {k: v for k, v in CATEGORY_LABELS.items() if k != 'linh-vuc-khac'}

print(f"üîÑ Generating QnA for {len(target_categories)} categories at once...")
raw_result = generate_all_categories(target_categories, N_SAMPLE, TEMPERATURE)

# Parse and Save
parsed_data = parse_gemini_output(raw_result, target_categories)

for slug, qna_list in parsed_data.items():
    if not qna_list:
        print(f"‚ö†Ô∏è Warning: No data for {slug}")
        continue

    checkpoint_file = os.path.join(SAVE_DIR, f"{slug}.json")
    save_json(qna_list, checkpoint_file)
    print(f"‚úÖ Saved {len(qna_list)} questions to {checkpoint_file}")

    combined_data.extend(qna_list)
    save_json(combined_data, os.path.join(SAVE_DIR, COMBINED_FILE))

print(f"üéâ Done! Total combined samples: {len(combined_data)}")
